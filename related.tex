\section{Related Work}
\label{sec_related_work}
% Our work is related to the document classification task in natural language processing (NLP). 
In document classification, a simple but effective method is to combine bag-of-words (BOW) features with varies classifiers~\cite{joachims1998text}. 
Recently, neural network (NN) models like Convolutional Neural Network (CNN)~\cite{kim2014convolutional} have been used for document embedding, and the resultant document vecotr can be further used for classification.
\cite{tang2015document} proposes a two-layer scheme, where they use recurrent neural network (RNN) or CNN for sentence embedding, and another RNN for document embedding.
Our method follows the two-layer scheme, and shares similar spirit with \cite{yang2016hierarchical} in using context vectors to distinguish informative words or sentences from non-informative ones, but instead of global context vectors, they can be dynamically generated for each datum when extra guidance is available.
We also differs from these works in using extracted relevant law articles to support charge classification, which require us to distinguish relevant articles from irrelevant ones, and further aggregate the information from two sources for classification.

As for multi-label document classification, two loss functions are commonly used. 
The first one is binary cross entropy~\cite{nam2014large}, which treats the multi-label classification task as multiple binary classification tasks. 
The second one is cross entropy~\cite{kurata2016improved}, which converts the multi-label target to label distribution during training, and use a threshold selected via validation set to generate the final prediction during testing. In our pilot experiments, we find the latter one converges faster and performs better, so we will use the latter one in this paper.

In the thread of work on charge prediction, \cite{LIU2004case,liu2006exploring} use KNN to classify charges of criminal cases in Taiwan. Except for the inferior scalability of the KNN method, the word-level or phrase-level features also do not provide enough information to distinguish very similar charges. \cite{lin2012exploiting} propose to make deeper understanding of the case by identifying key factors manually designed regarding two similar charges. Their method also suffers from sclability issue since the human efforts requied for designing and annotating these charge specific factors. Our method, however, employs GRU and attention mechnism to make comphrehensive understanding of the case, and all the training data are automatically constructed based on public judgement docuemnts. 

\cite{liu2005classifying,liu2006exploring} also try to find the specific law articles that has been violated. However, they convert the multi-label problem to multi-class classification problem by only considering a fixed set of article combinations, which can not scale well since the number of possible combinations will grow expentially when a larger set of law articles are considered.
\cite{liu2015predicting} instead designs a scalable way to find relevant law articles, 
by first use Support Vector Machine (SVM) for preliminary article classification, and then 
rerank the results by using the similarity between the words in facts and articles, and the correlations among law articles as indicators.
% use some reranking methods to get the final relevant article list. 
We also utilizes SVM to extract top $k$ articles, but instead use GRU and attention mechanism to better understand texts and the correlation among articles.
% However, to better understand the association between facts and articles, we use attention mechanism to distinguish relevant ones in the top $k$ extraction results. \orange{To make our model fully differenciable,} rather than using frequent pattern mining techinque, we use RNN to model the correlations among articles.

Another related thread of work is to predict the overall outcome of a case. The target can be which party will the outcome side with~\cite{aletras2016predicting}, or will the present court affirm or reverse the decision of a lower court~\cite{katz2016general}. Our work mainly differs from them in that, instead of binary outcome (the latter one also contains an \emph{other} class), we step further to focus on the detailed results of a case, i.e., the charges, where the output may contain multiple labels. 

% Another related thread of work is to predict the overall outcome of a case. The target can be predicting whether the outcome will side with the plaintiff or defendant~\cite{bruninghaus2003predicting,aletras2016predicting}, or will the present court affirm or reverse the decision of a lower court~\cite{martin2004competing,katz2016general}. Our work mainly differs from them in that, instead of binary outcome, we step further to focuse on the detailed results of the case, i.e., the charges, where the output may contain multiple labels. 


Our work also shares similar spirit with the legal question answering task~\cite{COLIEE14}, which aims at answering the yes/no questions in Japanese legal bar exams, that we all believe that relevant law articles are important for decisions in civil law system. 
The task first requires participants to extract relevant Japanese Civil Code articles, and then these articles are used to answer the yes/no question. 
The article extraction phase is often treated as an information retrieval task, and the question answering phase is usually considered as a textual entailment task~\cite{kim2014legal,kimconvolutional}
% Another thread of work also tries to answer the multiple-choice questions in the USA National Bar Exam \cite{FAWEI16,adebayoneural}. Since the United States mainly operates on common law system, where statutory laws are less important, the relevant article extraction phase is not employed in these works.