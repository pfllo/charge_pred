\section{Related Work}
\label{sec_related_work}
The charge prediction task aims at predicting appropriate charges based on the facts of a case.
\cite{LIU2004case,liu2006exploring} use KNN to classify charges of criminal cases in Taiwan. However, except for the inferior scalability of the KNN method, their word-level or phrase-level features also do not provide enough information to distinguish very similar charges. 
\cite{lin2012exploiting} proposes to make deeper understanding of a case by identifying charge-specific factors manually designed for two charges. Their method also suffers from scalability issue due to the human efforts requied to design and annotate these factors. Our method, however, employs GRU and attention mechnism to make comphrehensive understanding of a case, and all the training data are automatically constructed based on public judgement documents. 

On the other hand,
\cite{liu2005classifying,liu2006exploring} also try to find the specific law articles that has been violated. However, they convert the multi-label problem to multi-class classification problem by only considering a fixed set of article combinations, which cannot scale well since the number of possible combinations will grow expentially when a larger set of law articles are considered.
\cite{liu2015predicting} instead designs a scalable way to find relevant law articles, 
by first using Support Vector Machine (SVM) for preliminary article classification, and then 
rerank the results by using the similarity between the words in facts and articles, and the correlations among law articles as indicators.
% use some reranking methods to get the final relevant article list. 
We also utilizes SVM to extract top $k$ articles, but instead use GRU and attention mechanism to better understand the texts and the correlation among articles.
% However, to better understand the association between facts and articles, we use attention mechanism to distinguish relevant ones in the top $k$ extraction results. \orange{To make our model fully differenciable,} rather than using frequent pattern mining techinque, we use RNN to model the correlations among articles.

Another related thread of work in field of artificial intelligence and law is to predict the overall outcome of a case. The target can be predicting which party will the outcome side with~\cite{aletras2016predicting}, or will the present court affirm or reverse the decision of a lower court~\cite{katz2016general}. Our work mainly differs from them in that, instead of binary outcome (the latter one also contains an \emph{other} class), we step further to focus on the detailed results of a case, i.e., the charges, where the output may contain multiple labels. 

% Another related thread of work is to predict the overall outcome of a case. The target can be predicting whether the outcome will side with the plaintiff or defendant~\cite{bruninghaus2003predicting,aletras2016predicting}, or will the present court affirm or reverse the decision of a lower court~\cite{martin2004competing,katz2016general}. Our work mainly differs from them in that, instead of binary outcome, we step further to focuse on the detailed results of the case, i.e., the charges, where the output may contain multiple labels. 


We share similar spirit with the legal question answering task~\cite{COLIEE14}, which aims at answering the yes/no questions in Japanese legal bar exams, that we all believe that relevant law articles are important for decisions in civil law system. 
The task requires participants to first extract relevant Japanese Civil Code articles, and then use them to answer the questions. 
The article extraction phase is often treated as an information retrieval task, and the question answering phase is usually considered as a textual entailment task~\cite{kim2014legal,kimconvolutional}
% Another thread of work also tries to answer the multiple-choice questions in the USA National Bar Exam \cite{FAWEI16,adebayoneural}. Since the United States mainly operates on common law system, where statutory laws are less important, the relevant article extraction phase is not employed in these works.

Our work is also related to the task of document classification, but differs from it in that we also use automatically extracted law articles to improve and support the charge prediction.
% , a simple but effective method is to combine bag-of-words (BOW) features with varies classifiers~\cite{joachims1998text}. 
Recently, neural network (NN) models like Convolutional Neural Network (CNN)~\cite{kim2014convolutional} have been used for document embedding, and the resultant document vecotr is further used for classification.
\cite{tang2015document} proposes a two-layer scheme, where they use recurrent neural network (RNN) or CNN for sentence embedding, and another RNN for document embedding.
Our method also uses this two-layer scheme, and shares similar spirit with \cite{yang2016hierarchical} in using context vectors to distinguish informative words or sentences from non-informative ones, but instead of global context vectors, we allow them to be dynamically generated for each datum when extra guidance is available.
% We also differs from these works in using extracted relevant law articles to support charge classification, which require us to distinguish relevant articles from irrelevant ones, and further aggregate the information from two sources for classification.

As for multi-label classification, two loss functions are commonly used in natural language processing. 
The first one is binary cross entropy~\cite{nam2014large}, which treats the multi-label classification task as multiple binary classification tasks. 
The second one is cross entropy~\cite{kurata2016improved}, which converts the multi-label target to label distribution during training, and use a threshold selected on validation set to generate the final prediction during testing. In our pilot experiments, we find the latter one converges faster and performs better, so the latter one is used in this paper.