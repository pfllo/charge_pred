\section{Experiments}
\subsection{Experimental Setup}
We use HanLP\footnote{\url{https://github.com/hankcs/HanLP}} for Chinese word segmentation and POS tagging.
Word embeddings are trained using word2vec~\cite{mikolov2013distributed} on judgement documents, web pages from several legal forums and Baidu Encyclopedia. The resulting word embeddings contain 573,353 words, with 100 dimension.
% We use word2vec~\cite{mikolov2013distributed} to train our word embeddings with judgement documents, and web pages from several legal forums as well as Baidu Encyclopedia. The resulting word embeddings contain  573,353 words, with 100 dimension.  
%As for word embeddings, we use Baidu Encyclopedia, 3 million judgement documents and 3 million legal question answer pairs crawled from multiple legal forums as corpus, and the word2vec~\cite{mikolov2013distributed} for training. 
%The size of the resultant word embedding is 100-d and there are 573,353 words in total. 
% During training and testing, all the time expressions, names and charges\footnote{Although rare, sometimes the charge may appear in the fact part. This conversion ensures that we do not use this information.} in the text are converted to 3 special tokens separately and the words not in the pre-trained word embeddings are converted to another special token. All the word embeddings remain unchanged during training except for the special tokens. 
We randomly initialize a 50-d vector for each POS tag, which is concatenated with the word embedding as the final input.
Each GRU in the Bi-GRU is of size 75, the two full connection layers are of size 200 and 150.
The relevant article extractor generates top 20 articles, the weight of the article attention loss ($\beta$ in Eq.~\ref{final_loss}) is 0.1, and prediction threshold $\tau$ is 0.4.
We use Stochastic Gradient Descent (SGD) for training, with learning rate 0.1, and batch size 8.
% We also clip the gradient so that the sum of square of the L2 norms of the trainable tensors does not exceed 5. 
% To accelerate the training, we constrain the number of sentences of each fact description to be less than 50 and the the maximum length of each fact sentence to be 50. As for articles, we constrain the maximum length of each sentence to be 30. 
%The chi-square feature selector keeps the top 2,000 features.
% , and we use the SVM model implemented in scikit-learn \cite{scikit-learn} in our experiments.

We compare our full model with two variations: without article attention supervision and only using facts for charge prediction. We also implement an SVM model, which proves to be effective in many text classification tasks~\cite{wang2012baselines,aletras2016predicting}. Specifically, the SVM model takes bag-of-words TF-IDF features as input, and uses chi-square to select top 2,000 features.


\begin{table}
\centering
\small{
\begin{tabular}{|c|c|c|c|}
\hline
% \multirow{2}{|c|}{Model} & \multicolumn{3}{c|}{\tabincell{c}{ (\textit{Micro-/Macro-}) }}
\multirow{2}{*}{\textbf{Model}}				& \tabincell{c}{\textbf{Precision}} 	& \tabincell{c}{\textbf{Recall}} 		& \tabincell{c}{\textbf{F1}} 	\\
% \hline
\cline{2-4}
                                               & \multicolumn{3}{c|}{\tabincell{c}{ (\textit{Micro-/Macro-}) }}\\
\hline
\textit{SVM} 				& \textbf{93.94}/79.53					& 77.66/49.54  					& 85.03/61.05 				 	\\
\hline
\textit{SVM\_article} 			& 91.77/71.33					& 72.10/45.85  					& 80.76/55.82				 	\\
\hline
\textit{NN}				& 91.30/\textbf{83.32}			& 87.39/74.99  					& 89.31/78.94					\\
\hline
\textit{NN\_article\_only} 			& 90.09/81.50				& 86.10/69.62				& 88.05/75.10		\\
\hline
\textit{NN\_article}			& 90.79/83.07					& 88.42/75.73  					& 89.59/79.23					\\
\hline
\textbf{\textit{NN\_supv\_article}} 	& 91.80/82.44 					& \textbf{88.67/78.62} 			& \textbf{90.21/80.48} 		 	\\
\hline
\hline
\textit{SVM\_gold\_article$^*$} 	& \textbf{98.97}/94.58			& 95.39/83.21  					& 97.15/88.53					\\
\hline
% \textit{SVM\_only\_gold$^*$} 		& 98.78/90.46					& 97.92/91.79  					& 98.35/91.12					\\
% \hline
\textbf{\textit{NN\_gold\_article$^*$}} 		& 98.78/\textbf{95.26} 			& \textbf{98.24/95.57} 			& \textbf{98.51/95.42} 			\\
\hline
% \hline
% \textit{NN\_article\_only} 			& 90.09/81.50				& 86.10/69.62				& 88.05/75.10		\\
% \hline
% \textit{NN\_gold\_only$^*$} 		& 97.22/92.39				& 98.36/94.73				& 97.79/93.55		\\
% \hline
\end{tabular}
}
% \vspace{-.5em}
\caption{Charge prediction results. $^*$ refers to using gold standard articles during training and testing. Left and right side of the slash refer to micro and macro statistics, respectively.}
\label{tabble_main_results}
\end{table}


% \textit{NN\_article\_only} 			& 87.38/78.61				& 82.69/64.32				& 84.97/70.75		\\


\subsection{Charge Prediction Results}
\label{sec_main_results}
%The charge prediction results are summarized in Table \ref{tabble_main_results}. 
% The left side of the slash refers to micro statistics, and the right side refers to macro statistics. 
% Micro precision (or recall) is the the number of correct predictions divided by the total number of predictions (or the total number of gold standard charges), while the macro precision (or recall) is the sum of the precision (or recall) of each charge divided by the number of distinct charges. The micro (or macro) F1 is the harmonic mean of the micro (or macro) precision and recall.
We evaluate the charge prediction task using precision, recall and F1, in both micro- and macro-level.
The macro-precision/recall are calculated by averaging over %the precision/recall of 
each charge, and the micro ones are averaged  over each prediction. %In other words, macro statistics give equal weight to each charge, while micro statistics give equal weight to each individual prediction. The micro (or macro) F1 is the harmonic mean of the micro (or macro) precision and recall.


%The $Acc.$ refers to instance accuracy, which is the number of instances whose classes are all correctly predicted divided by the total number of instances.

As shown in Table~\ref{tabble_main_results}, %we can see that, 
the basic \texttt{SVM} model,
which only takes fact descriptions as input, indeed proves to be a strong baseline.
By contrast, our corresponding neural network model (\texttt{NN}), which also only uses facts for prediction, outperforms \texttt{SVM} 
by around 4\% in micro-F1.
Since \texttt{NN} benefits from the pre-trained word embeddings,
a two-level Bi-GRU architecture, and the fact-side attention module,
it can thus attentively recognize informative expressions from the description and 
better capture the underlying correspondence from fact descriptions to appropriate charges,
even when there is less overlap in the words used among cases with the same charge,
or when there are limited training data (i.e., infrequent charges). This may also explain that NN models 
have more balanced performance over different charges, leading to more prominent improvements over 
SVM ones in macro metrics, which usually have a strong bias towards frequent charges.
%

%This actually indicates the nature of the civil law system that judgements are made based on statutory laws rather than decisions of previous cases.

When we use both the facts and the extracted relevant law articles (that are admittedly noisy),
the SVM version (\texttt{SVM\_article}) drops by around~5\% than \texttt{SVM}, showing that the SVM model 
cannot benefit from the extracted, thus noisy, relevant articles in such a straightforward way.
However, our NN version (\texttt{NN\_article}) can still learn from  the noisy article extractions through 
attentively aggregating those extracted articles even without direct guidance,
thus improves \texttt{NN} by around 0.4\%.
Furthermore, if we use the gold standard articles during training as supervision for the 
article attention (our full model, \texttt{NN\_supv\_article}), the performance can be further improved, achieving 
90.21\% and 80.48\% in micro- and macro-F1, respectively.
%This proves that it is important to take relevant law articles into considerations for charge prediction.
The improvements made by using relevant law articles actually indicates the nature of the civil law system that judgements are made based on statutory laws rather than decisions of previous cases.

However, if we only use the extracted relevant articles to make prediction (\texttt{NN\_article\_only}\footnote{
\texttt{NN\_article\_only} utilizes fact embeddings to attentively aggregate relevant articles, but only use 
the aggregated article embedding $\mathbf{d}_a$, without fact embedding  $\mathbf{d}_f$,  for charge prediction.}),
even with the proved-helpful attentive aggregator, the model performs worst among all NN 
variants (though still better than \texttt{SVM}). This indicates that  it is necessary to consider both facts and 
relevant law articles for charge prediction, and,
the fact that \texttt{NN} outperforms \texttt{NN\_article\_only} also indicates that although the judgments are made based on the statutory laws in the civil law system,
the logic %\textbf{logical thinking or legal reasoning} 
employed by the court when making decisions,
% of the court behind the formal decisions, 
to some extent, may be implicitly captured through massive fact-charges paris.



Now the question is: \textit{how much improvement can we have 
% by making full use of %expoiting 
if we can make full use of
the relevant law articles within the civil law system?}
% \orange{Now the question is: \textit{what is the upper bound of the improvements that can be made by using relevant law articles within the civil law system?}}
Let us consider an ideal situation where we can access both fact descriptions and gold standard law articles during testing, which could be considered as an upper bound scenario.
The SVM version (\texttt{SVM\_gold\_article}) significantly outperforms %\texttt{SVM} by around 27\% in macro-F1, and especial
\texttt{SVM\_article} by more than 30\% in macro-F1.
And the NN version %upper-bound 
(\texttt{NN\_gold\_article}) outperforms 
% our best article version (\texttt{NN\_supv\_article}) 
\texttt{NN\_supv\_article}
by over 8\%.
% , and no surprisingly, \texttt{NN\_article} by more than 10\%.
%
These comparisons confirm again that 
law articles play an important role for automatic judgement prediction, but 
the extracted relevant articles inevitably contain noise, which should be properly 
handled, e.g., using an attentively aggregation mechanism to distill 
valuable evidence to support the charge prediction.



%If we can use both the facts and the gold standard articles of each case during testing (\texttt{SVM\_gold\_article}), the performance will be significantly improved, showing that the relevant law articles contain valuable information for charge prediction. However, the results become worse when we replace the gold standard articles with the extracted ones (\texttt{SVM\_article}), indicating that the simple SVM model cannot benefit from the extracted, thus possibly noisy, relevant articles in a straightforward way. 
% If we only use the gold standard articles of each case for classification (\texttt{SVM\_only\_gold}), the micro F1 can be further improved to 98.35, which further indicates that the SVM model has bad resistence to noise. \todo{is \texttt{\_only\_gold} necessary?}

%On the other hand, since our attentive article aggregator has the ability to distinguish relevant articles from \red{extraction errors/irrelevant ones}, our neural network (NN) model (\texttt{NN\_article}) can learn from  the noisy article extractions, and improve the performance over the model using facts only (\texttt{NN}), which has already outperfoms \texttt{SVM} by a large margin.

%Furthermore, if we use the gold standard articles during training to \orange{supervise} the article attention (\texttt{NN\_supv\_article}), the performance can be further improved.
%Similar to the SVM case, if we can access gold standard articles during testing (\texttt{NN\_gold\_article}),% instead of extracted ones, 
%there will be a clear improvement as well, which actually indicates the upper bound of the improvement that relevant articles can bring to our model.

% and without surprise, it also outperforms \texttt{SVM\_gold\_article}.
%Also note that, compared with the corresponding SVM models, the improvements made by our NN architecture are more prominent in macro statistics, since SVM has a strong bias towards frequent charges, while our NN models have a more balanced improvement in both frequent and infrequent ones. 
%
%Our NN models harness the pre-trained word embeddings \red{with two-stack attention mechanism????} to effectively encode sentences and documents, thus can better capture the underlying correspondence from fact descriptions to appropriate charges, \red{even there is less overlap in the words used, or there are limited training data for infrequent charges.  }
%\orange{This is probably due to the embedding methods used by our NN models. By using pre-trained word embeddings and training effective sentence-level and document-level sequence encoders, our model can capture the meaning of the cases belonging to infrequent charges even if their training data are limited, and thus performs better on these cases.}
% This is probably due to the nature of the BOW feature, which is unable to handle synonyms. However, understanding synonyms is crucial for inferring common patterns when training data are limited. Therefore, by using pre-trained word embeddings to handle synonyms, our NN model perfroms well on infrequent charges. 
% it can better understand the meaning of a word even when it is rare in the training data. 

% As for our neural network (NN) model (\texttt{NN}), since it can better understand the associations among sentences and words, and can select the most informative parts of the text, they significantly outperforms the corresponding SVM models. 

%We also evaluate two variations which only use articles for charge prediction.
%\texttt{NN\_gold\_only} uses concatenated gold standard articles, instead of facts, as input. 
%\texttt{NN\_article\_only} uses extracted articles, and it still uses fact embedding and article aggregator to handle the noise in the top 20 articles, but only the aggregated article embedding $\mathbf{d}_a$ is used for charge prediction.
%We can see that the performance of \texttt{NN\_gold\_only} is very good and \texttt{NN\_article\_only} also generates reasonable results. 
%This actually indicates the nature of the civil law system that judgements are made based on statutory laws rather than decisions of previous cases.
%Also note that \texttt{NN\_gold\_only} is not as good as \texttt{NN\_gold\_article}, showing that the facts can provide additional information for charge prediction even if we use gold standard law articles.
%On the other hand, the performance of \texttt{NN} is also promising and outperforms \texttt{NN\_article\_only}, showing that although the judgments are based on statutory laws, we can still learn the \orange{legal logic} behind this implicitly through only the (facts, charges) paris. 

% Specifically, the relevant articles are concatenated and passed to the model by replacing the fact descriptions. We find that using only the top 20 extracted articles (\texttt{NN\_article\_only}) still generates \orange{fair} results, and the model performs very good when the extracted articles are replaced with gold standard ones (\texttt{NN\_gold\_only}), which significantly outperforms \texttt{NN}. 



% \orange{And the big gap between \texttt{NN\_supv\_article} and \texttt{NN\_gold\_article} indicates that there is still much room that our article extraction and embedding part can improve.}

% Recall that using only facts as input, the results of \texttt{NN} is promising, showing that although the legal system of China is mainly based on civil law system, the judgements are still consistent with the facts enough for our model to implicitly learn the legal logic behind the judgement.
% which to some extent indicates a reasonable consistency of Chinese judges in making judgements. 

\paragraph{Case Study}
We study the model outputs and find certain star-like confusion patterns among the charges. For example, \emph{intentional injury} %(\emph{pivot charges}) 
%multiple charges (\emph{peripheral charges}) like 
is often confused with multiple charges like 
\emph{intentional homicide} (when the victim is dead, the difference is %it differs from \emph{intentional injury} in
 whether the defendant intends to kill  or just hurt the victim) and \emph{picking quarrels and provoking troubles}
(there may also exist injuries here).
% We refer to charges can be confused with multiple charges as \emph{pivot charges}, and the charge often confused with a single charge as \emph{peripheral charges} here.
These charges usually share some similar fact descriptions, e.g., how the injuries are caused, and since \emph{intentional injury} appears more frequently than the others, \texttt{SVM} thus outputs \emph{intentional injury} in most situations, and fails to distinguish these charges.
However, by using Bi-GRU and the attention mechanism, \texttt{NN} can attend to important details of the facts and significantly improves the performance on these charges.
%On the other hand, these learned patterns are not reliable enough, and lead to limited improvements on pivot charges.
When the direct supervision for articles is available,  \texttt{NN\_supv\_article}  can enhance the interaction between certain pairs of descriptions and law articles, which helps to capture the subtle differences among similar charges, and further improves the performance.
%and therefore learns more elaborated patterns, which significantly improve the pivot charges while less prominent improvements on other easily confused charges are made as well.

% On the other hand, while \texttt{NN\_supv\_article} also improves \texttt{NN} on these this kind of charges, a \orange{considerable} amount of improvements comes from the charges that can be confused with multiple charges, e.g., \emph{intentional injury} is also often confused with \emph{picking quarrels and provoking troubles} since there will also be injuries in the latter one, indicating that the additional information from relevant law articles can help the model handle the difficult charges better.


\subsection{Article Extraction Results}
%\paragraph{Top $k$ Article Extraction}
% \begin{table}
% \centering
% \normalsize{
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% 				& \textbf{Top\_5} 	& \textbf{Top\_10} 		& \textbf{Top\_20} 	& \textbf{Top\_30} \\
% \hline
% \textit{Recall} 		& 77.60			& 88.96  				& 94.21			& 96.53 	\\
% \hline
% \textit{NDCG} 		& 80.28			& 84.32  				& 86.47			& 87.24 	\\
% \hline
% \end{tabular}
% }
% \caption{Top $k$ Article Extraction Performance}
% \label{tab_article_extraction}
% \end{table}
We also evaluate our SVM article extractor, which achieves 
%Our top $k$ article extractor achieves 
77.60\%, 88.96\%, 94.21\% and 96.53\% recall regarding the top 5, 10, 20 and 30 articles, respectively.
Although simple, the SVM extractor can obtain over 94\% recall for top 20 articles, which is good enough for further refinement.
However, the micro F1 of the extractor is only 61.08\% in the test set, which will lead to severe error propagation problem if we use them directly. Therefore, we design the article attention mechanism to handle the noise in the top 20 articles.

% Our top $k$ article extractor achieves 94.21\% recall for the top 20 results, which is good enough to for the following refinement components. However, the recall of the top 5 results is 77.60\% and the micro F1 is only 61.08\%, which indicates that we still cannot trust its prediction results directly, and therefore need to expand the number of results to ensure enough recall for the following components. Furtherfore, we find that the recall of the top 30 results is only 2.32\% higher than the top 20 results, therefore we use the top 20 results in this paper.

% Our relevant article extraction module achieves 86.44\% top 1 accuracy, 61.08\% micro F1, and the other evaluation indicators are shown in Table \ref{tab_article_extraction}. 
% We can see that, the relevant article extraction task is also a hard task in itself. Although our SVM model achieves reasonable ranking performance, its prediction performance is not very good. If we use the prediction results directly in our model, we will suffer from a severe error propagation problem. Therefore, we instead let the relevant article extraction module return the top $k$ articles, and use attention mechanism the distinguish true relevant articles from incorrect ones. Since the recall of the top 20 results has achieved 94.21\%, which already includes most of the relevant articles, we use top 20 articles in our experiment.

\begin{table}
\centering
\normalsize{
\begin{tabular}{|c|c|c|c|c|}
\hline
% \textbf{Att\_Weight}
% \bm{$\beta$}
\bm{$\beta$}							& \textbf{Prec@1} 		& \textbf{MAP} 			& \textbf{Charge\_F1} \\
\hline
\textit{0} 								& 60.94								& 61.61 						& 89.59/79.23 	\\
\hline
\textit{0.01} 						& 81.06								& 78.00							& 89.77/79.48 	\\
\hline
\textit{0.1} 							& 87.90								& 83.39							& \textbf{90.21/80.48} 	\\
\hline
\textit{0.5} 							& 91.44								& 86.95							& 89.93/79.67 	\\
\hline
\textit{1} 								& \textbf{92.66}			& \textbf{88.24}		& 89.83/78.66 	\\
\hline
\end{tabular}
}
\caption{Refined Article Extraction Performance}
\label{tab_article_att}
\end{table}


%\paragraph{Article Attention}
We can also evaluate the extracted articles after re-ranking by our article attention module.
%We can consider the article attention module as a re-ranking function over the extracted top $k$ articles, and accordingly use the gold standard articles in the $k$ articles to evaluate the re-ranking performance. 
Table \ref{tab_article_att} shows the refined extraction results  (column 2-3) and the corresponding charge prediction performances (column 4), under different weights for article attention ($\beta$ in Eq.~\ref{final_loss}). \texttt{Prec@1} refers to top 1 precision, and \texttt{MAP} refers to mean average precision.
%
We can see that, even if there is no supervision over the article attention ($\beta=0$), our model still has reasonable performance on re-ranking the $k$ articles. When the attention supervision is employed, the extraction quality improves significantly, and keeps increasing as $\beta$ goes up.
%The \orange{fair} performance of the article attention indicates that our model can provide reasonable legal basis to support the charge prediction.
However, the charge prediction performance does not always increase with the article extraction quality, and the best performance is achieved when $\beta=0.1$. This is not surprising, since  there exists a tradeoff between the benefits of more accurate article extraction and the less model capacity left for charge classification due to the increased emphasis on the article extraction performance.

\begin{table}
\centering
\small{
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model}												& \textbf{Precision} 				& \textbf{Recall} 				& \textbf{F1} 	\\
\hline
\textit{SVM} 													& \textbf{100.00}						& 40.20  									& 57.34 				 	\\
\hline
\textit{NN}														& 87.14											& 59.80 									& 70.93					\\
\hline
\textit{NN\_article}					& 87.18											& 66.67 									& 75.56					\\
\hline
\textbf{\textit{NN\_supv\_article}} 	& 90.00 										& \textbf{70.59} 					& \textbf{79.12} 		 	\\
\hline
\end{tabular}
}
\caption{Performance (micro statistics) on News Data}
\label{tabble_news_results}
\end{table}

\subsection{Performance on News Data}
There are usually 
clear %significant 
differences between the expressions used by legal practitioners and people without legal background, 
thus it is important to see how our model will perform on fact descriptions written by non-legal professionals.
%
%We evaluate our models  on news reports that are composed by general reporters,
%where 
We then manually annotate 100 news reports %by general reporters 
from two news websites\footnote{\url{news.cn/legal} and \url{legal.people.com.cn}},
containing 262 words on average and 25 distinct charges.
%
The results are shown in Table \ref{tabble_news_results}, where we only report micro statistics due to 
the small size of the dataset.
% the relatively small size of the dataset compared with the number of distinct charges.
% since macro ones are not 

%Also note that the size of the news test set is relatively small with regard to the number of charges, which results in less reliable macro statistics, we therefore only report  here.

We can see that, \texttt{SVM} suffers a significant drop in F1 on the news data, %compared to on judgement documents,
confirming the gap between the expressions used by legal practitioners and non-legal professionals,
given the BOW nature of \texttt{SVM}.
Although \texttt{SVM} cannot generalizes well, the patterns learned by \texttt{SVM} are reliable in themselves, leading to a high precision.
It is not surprising that our NN models also suffer from the expression differences, %\orange{news genre}, 
but due to the effectiveness of our NN architecture, with about 10\%$\sim$15\% less absolute drop in F1, and \texttt{NN\_supv\_article} can still achieve 79.12\% in F1. %, \orange{thanks to our NN architectures}.
%exists a performance drop, they are much better than the SVM model. This shows that the  gap in expressions can be resolved by using embedding methods to a some extent. 
For example, the word 暴打$\ $(beat up) is seldom used in judgement documents,
making it hard for \texttt{SVM} to correctly utilize  暴打$\ $as an indicator for injury related charges, but, %However, by using pre-trained word embeddings, 
our NN models can associate it with its  near-synonymy  殴打$\ $(beat), which is a formal expression in judgement documents.
Furthermore, the clear improvements from \texttt{NN} to \texttt{NN\_article}, and further to \texttt{NN\_supv\_article} prove again the importance of relevant articles 
in supporting charge prediction, even in news domain.
%we can see that the model using relevant articles (\texttt{\_article}) clearly outperforms \texttt{NN}, \orange{showing that relevant articles also provide valuable information in this situation.}

% Also note that, since the news test set is relatively small regarding the number of charges, the macro statistics are not as meaningful as those in the judgement document test set. For example, correctly predicting a charge with only one instance will improve the macro precision and recall by 4\% (${1}\div{25}$). Therefore, \texttt{NN\_article} outperforms \texttt{NN\_supv\_article} in macro F1 by 4.38\% does not necessarily mean that it performs better on infrequent charges. 
% Actually, %except for other differences, 
% \texttt{NN\_article} only generates one more such case than \texttt{NN\_supv\_article}, while \texttt{NN\_supv\_article} correctly predicts 4 more cases than \texttt{NN\_article} in total. Therefore, we still consider \texttt{NN\_supv\_article} to be better than \texttt{NN\_article} on news data.


% Also note that compared with \texttt{NN\_article}, \texttt{NN\_supv\_article} has better micro statistics but worse macro statistics, indicating that the additional supervision over article attention makes the model perform better on frequent charges but worse on infrequent ones on news data. 
% The reason is that, since the top $k$ article extractor also utilizes BOW features, its performances drops significantly on news data, leading to some situations where the key article is not included in the top $k$ results. While the attention of \texttt{NN\_article} will be diffusive in these situations, due to the article attention supervision, \texttt{NN\_supv\_article} has a stronger tendency to put the attention on the articles related to frequent or other relevant charges, and thus is more likely to generate false prediction.  
% \todo{re-organize this part}

