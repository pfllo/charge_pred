\section{Experiments}
\subsection{Implementation Details}
We use HanLP\footnote{\url{https://github.com/hankcs/HanLP}} for Chinese word segmentation and POS tagging. 
As for word embeddings, we use the summary part of Baidu Encyclopedia, 3 million judgement documents and 3 million legal question answer pairs crawled from multiple legal forums as corpus, and the word2vec~\cite{mikolov2013distributed} for training. 
The size of the resultant word embedding 100-d and there are 573,353 words in total. 
% During training and testing, all the time expressions, names and charges\footnote{Although rare, sometimes the charge may appear in the fact part. This conversion ensures that we do not use this information.} in the text are converted to 3 special tokens separately and the words not in the pre-trained word embeddings are converted to another special token. All the word embeddings remain unchanged during training except for the special tokens. 
We also randomly initialize a 50-d vector for each POS tag, and the POS tag embedding is concatenated with the word embedding to generate the final input. 
Each GRU in the Bi-GRU is of size 75, the two full connection layers are of size 200 and 150.
We keep the top 20 articles generated by the top $k$ article extractor, and the weight of the article attention loss is 0.1. 
We use Stochastic Gradient Descent (SGD) for training, with learning rate 0.1, and batch size 8.
% We also clip the gradient so that the sum of square of the L2 norms of the trainable tensors does not exceed 5. 
% To accelerate the training, we constrain the number of sentences of each fact description to be less than 50 and the the maximum length of each fact sentence to be 50. As for articles, we constrain the maximum length of each sentence to be 30. 
The feature selector in the top $k$ article extractor model keeps the top 2,000 features.
% , and we use the SVM model implemented in scikit-learn \cite{scikit-learn} in our experiments.


\begin{table}
\centering
\small{
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model}				& \textbf{Precision} 	& \textbf{Recall} 		& \textbf{F1} 	\\
\hline
\textit{SVM} 				& \textbf{93.94}/79.53					& 77.66/49.54  					& 85.03/61.05 				 	\\
\hline
\textit{SVM\_article} 			& 91.77/71.33					& 72.10/45.85  					& 80.76/55.82				 	\\
\hline
\textit{NN}				& 91.30/\textbf{83.32}			& 87.39/74.99  					& 89.31/78.94					\\
\hline
\textit{NN\_article}			& 90.79/83.07					& 88.42/75.73  					& 89.59/79.23					\\
\hline
\textbf{\textit{NN\_guide\_article}} 	& 91.80/82.44 					& \textbf{88.67/78.62} 			& \textbf{90.21/80.48} 		 	\\
\hline
\hline
\textit{SVM\_gold\_article$^*$} 	& \textbf{98.97}/94.58			& 95.39/83.21  					& 97.15/88.53					\\
\hline
% \textit{SVM\_only\_gold$^*$} 		& 98.78/90.46					& 97.92/91.79  					& 98.35/91.12					\\
% \hline
\textbf{\textit{NN\_gold\_article$^*$}} 		& 98.78/\textbf{95.26} 			& \textbf{98.24/95.57} 			& \textbf{98.51/95.42} 			\\
\hline
\hline
\textit{NN\_article\_only} 			& 87.38/78.61				& 82.69/64.32				& 84.97/70.75		\\
\hline
\textit{NN\_gold\_only$^*$} 		& 97.22/92.39				& 98.36/94.73				& 97.79/93.55		\\
\hline
\end{tabular}
}
\caption{Main Results. $^*$ means we use gold standard relevant law articles, which are not available in \orange{production environment}.}
\label{tabble_main_results}
\end{table}


\subsection{Main Results}
\label{sec_main_results}
We compare our method with a baseline BOW-based SVM method, which is similar to the one used for top $k$ article extractor, except that the outputs are charges rather than articles. The results are summarized in Table \ref{tabble_main_results}. The left side of the slash refers to the micro statistics, and the right side refers to the macro statistics. Specifically micro precision (or recall) is the the number of correct charge predictions divided by the total number of charge predictions (or the total number of gold standard charges), while the macro precision (or recall) is the sum of the precision (or recall) of each charge divided by the number of distinct charges in test set. The micro (or macro) F1 is the harmonic mean of the micro (or macro) precision and recall.


%The $Acc.$ refers to instance accuracy, which is the number of instances whose classes are all correctly predicted divided by the total number of instances.

We can see that, the basic \texttt{SVM} model, which only use facts as input, proves to be a strong baseline which achieves 85.03 micro F1 in our test set. If we use both the facts and the gold standard articles of each case (\texttt{SVM\_gold\_article}), the performance can be further improved significantly, showing that the relevant law articles contain valuable information for charge prediction. However, if we replace the gold standard articles with the extracted ones (\texttt{SVM\_article}), the results become rather worse, indicating that the SVM model cannot benefit from the additional information of relevant articles when they are noisy. 
% If we only use the gold standard articles of each case for classification (\texttt{SVM\_only\_gold}), the micro F1 can be further improved to 98.35, which further indicates that the SVM model has bad resistence to noise. \todo{is \texttt{\_only\_gold} necessary?}

On the other hand, since our attentive article aggregator has the ability to distinguish true relevant articles from extraction mistakes, our neural network (NN) model (\texttt{NN\_article}) can make use of the noisy top 20 article extractions, and improves the performance over the model using only facts as input (\texttt{NN}), which has already ourperfroms \texttt{SVM} by a large margin.
Furthermore, if we use the gold standard articles in the training data to guide the article attention (\texttt{NN\_guided\_article}), the performance can then be further improved.
Similar to SVM models, if we use gold standard articles (\texttt{NN\_gold\_article}) instead of extracted ones, there will exist a clear improvement as well, and without surprise, it also outperforms \texttt{SVM\_gold\_article}.
Also note that, compared with the corresponding SVM models, the improvements made by our NN architecture are most prominent in macro statistics, showing that the the SVM model has a \orange{strong} bias towards frequent charges, while the NN models have a more balanced performance between frequent and infrequent ones. This is probably due to the nature of BOW feature, which is unable to handle synonyms. However, since our NN model takes pre-trained word embeddings as input, it can better understand the meaning of a word even when it is rare in the training data. 

% As for our neural network (NN) model (\texttt{NN}), since it can better understand the associations among sentences and words, and can select the most informative parts of the text, they significantly outperforms the corresponding SVM models. 

We also evaluate our NN model with only the relevant articles as input. Specifically, the relevant articles are concatenated and passed to the model by replacing the fact descriptions. We find that using only the top 20 extracted articles (\texttt{NN\_article\_only}) still generates \orange{fair} results, and the model performs very good when the extracted articles are replaced with gold standard ones (\texttt{NN\_gold\_only}), which significantly outperforms \texttt{NN}. 
This actually indicates the nature of civil law system that judgements are made based on statutory laws rather than decisions of previous cases.
However, we can see that the performance of \texttt{NN} is also promising, showing that although the judgments are made based on statutory laws, our model can still learn the legal logic behind this implicitly through only the (facts, charges) paris. 
Also note that \texttt{NN\_gold\_only} is not as good as \texttt{NN\_gold\_article}, which shows that the facts can provide additional information for charge prediction even if we use gold standard law articles.
\orange{And the big gap between \texttt{NN\_guided\_article} and \texttt{NN\_gold\_article} indicates that there is still much room that our article extraction and embedding part can improve.}

% Recall that using only facts as input, the results of \texttt{NN} is promising, showing that although the legal system of China is mainly based on civil law system, the judgements are still consistent with the facts enough for our model to implicitly learn the legal logic behind the judgement.
% which to some extent indicates a reasonable consistency of Chinese judges in making judgements. 


\subsection{Performance of Top $k$ Article Extraction}
\begin{table}
\centering
\normalsize{
\begin{tabular}{|c|c|c|c|c|}
\hline
				& \textbf{Top\_5} 	& \textbf{Top\_10} 		& \textbf{Top\_20} 	& \textbf{Top\_30} \\
\hline
\textit{Recall} 		& 77.60			& 88.96  				& 94.21			& 96.53 	\\
\hline
\textit{NDCG} 		& 80.28			& 84.32  				& 86.47			& 87.24 	\\
\hline
\end{tabular}
}
\caption{Top $k$ Article Extraction Result}
\label{tab_article_extraction}
\end{table}

The performance of our top $k$ article extractor is show in Table \ref{tab_article_extraction}.
We can see that, although simple, our SVM article extractor already achieves promising performance, and the recall of the top 20 results has achieved 94.21\%, which is good enough for further article embedding and aggregation. 
On the other hand, however, we still cannot trust its prediction results directly.  In our test set, the micro F1 of the model is only 61.08\%, which will lead to severe error propagation problem, if we use the precition directly. Therefore, we instead retain the top 20 results, and use the powerful NN model to resolve the noise in the top 20 results.

% Our relevant article extraction module achieves 86.44\% top 1 accuracy, 61.08\% micro F1, and the other evaluation indicators are shown in Table \ref{tab_article_extraction}. 
% We can see that, the relevant article extraction task is also a hard task in itself. Although our SVM model achieves reasonable ranking performance, its prediction performance is not very good. If we use the prediction results directly in our model, we will suffer from a severe error propagation problem. Therefore, we instead let the relevant article extraction module return the top $k$ articles, and use attention mechanism the distinguish true relevant articles from incorrect ones. Since the recall of the top 20 results has achieved 94.21\%, which already includes most of the relevant articles, we use top 20 articles in our experiment.

\begin{table}
\centering
\normalsize{
\begin{tabular}{|c|c|c|c|c|}
\hline
% \textbf{Att\_Weight}
% \bm{$\beta$}
\bm{$\beta$}			& \textbf{Prec@1} 			& \textbf{Prec\_Full} 			& \textbf{MAP} 			& \textbf{Charge\_F1} \\
\hline
\textit{0} 								& 60.94									& 50.76  									& 61.61 						& 89.59/79.23 	\\
\hline
\textit{0.01} 						& 76.34									& 66.66  									& 76.24							& 89.94/80.13 	\\
\hline
\textit{0.1} 							& 87.90									& 74.14  									& 83.39							& \textbf{90.21/80.48} 	\\
\hline
\textit{1} 								& \textbf{92.66}				& \textbf{80.43}  				& \textbf{88.24}		& 89.83/78.66 	\\
\hline
\end{tabular}
}
\caption{Article Attention Evaluation}
\label{tab_article_att}
\end{table}


\subsection{Evaluating Article Attention}
We can consider the article attention module as a re-ranking function over the extracted top $k$ articles, and accordingly use the gold standard articles in the top $k$ articles to evaluate the re-ranking performance. Table \ref{tab_article_att} shows the model performance on article attention (column 2-4) and charge prediction (the last colum), (\orange{use comma?}) under different article attention loss weights ($\beta$ in Equation \ref{final_loss}). \texttt{Prec@1} refers to top 1 precision, and \texttt{Prec\_Full} refers to full precision. For example, if there are $k'$ gold standard articles in the extracted top $k$ articles, then \texttt{Prec\_Full} refers to the precision of the top $k$ articles.

We can see that, even if there is no guidance over the article attention ($\beta=0$), our model still has reasonable performance over these evaluation indicators. When attention guidance is added, the attention quality improves significantly, and it keeps increasing as $\beta$ goes up.
The \orange{fair} performance of the article attention indicates that, when predicting charges, our model can provide reasonable \orange{legal support} as well.
However, the charge prediction performance does not always increase with the article attention quality, and the best result is achieved when $\beta=0.1$. This shows that there exists a tradeoff between the benefits of more accurate article attention and the less model capacity left for charge classification due to the increased emphasis on the article attention performance.

\begin{table}
\centering
\small{
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model}												& \textbf{Precision} 				& \textbf{Recall} 				& \textbf{F1} 	\\
\hline
\textit{SVM} 													& \textbf{100.00}/56.00			& 40.20/34.64  						& 57.34/42.80 				 	\\
\hline
\textit{NN}														& 87.14/71.30								& 59.80/54.92 						& 70.93/62.05					\\
\hline
\textbf{\textit{NN\_article}}					& 87.18/\textbf{75.24}			& 66.67/\textbf{60.37} 		& 75.56/\textbf{66.99}					\\
\hline
\textbf{\textit{NN\_guide\_article}} 	& 90.00/68.72 							& \textbf{70.59}/57.50 		& \textbf{79.12}/62.61 		 	\\
\hline
\end{tabular}
}
\caption{Results on News Data}
\label{tabble_news_results}
\end{table}

\subsection{Performance on News Data}
Since there are some differences between the words used by \orange{laymen} and legal practitioners, 
we manually annotated 100 news\footnote{the news is randomly selected from \url{http://www.news.cn/legal/} and \url{http://legal.people.com.cn/}}, 
which contains an average of 261.79 words and 25 distinct charges, to see how the model trained on judgement documents performs on the fact descriptions written by \orange{ordinary people}. The results are shown in Table \ref{tabble_news_results}.

We can see that although the SVM model has good performance on the judgement document data, it suffers from a significant performance drop on news data.
Recall that the SVM model is based on BOW, this performance drop actually indicates an innegligible word usage gap between laymen and legal practitioners. 
However, when the key words match the words in training data, the SVM model is actually very reliable, which achieves 100\% micro precision on news tata.
As for our NN models, although there also exists a performance drop, they performs much better than the SVM model. This shows that the word usage gap can be resolved by using word embeddings to a some extent. Furthermore, we can see that the model using relevant articles (\texttt{\_article}) clearly outperform \texttt{NN}, showing that relevant articles provide valuable information in this situation.

Also note that, since the news test set is relatively small with regard to the number of charges, the macro statistics are not as meaningful as those in the judgement document test set. For example, correctly predicting a charge with only one instance will improve the macro precision and recall by 4\% ($1\div25$). Therefore, \texttt{NN\_article} outperforms \texttt{NN\_guide\_article} in macro F1 by 4.38\% does not necessarily mean that it performs better on infrequent charges. Actually, except for other differences, \texttt{NN\_article} actually only generates one more such case than \texttt{NN\_guide\_article}, while \texttt{NN\_guide\_article} correctly predicts 4 more cases than \texttt{NN\_article} in total. Therefore, we still consider \texttt{NN\_guide\_article} to be better than \texttt{NN\_article} on news data.


% Also note that compared with \texttt{NN\_article}, \texttt{NN\_guide\_article} has better micro statistics but worse macro statistics, indicating that the additional guidance over article attention makes the model perform better on frequent charges but worse on infrequent ones on news data. 
% The reason is that, since the top $k$ article extractor also utilizes BOW features, its performances drops significantly on news data, leading to some situations where the key article is not included in the top $k$ results. While the attention of \texttt{NN\_article} will be diffusive in these situations, due to the article attention guidance, \texttt{NN\_guide\_article} has a stronger tendency to put the attention on the articles related to frequent or other relevant charges, and thus is more likely to generate false prediction.  
% \todo{re-organize this part}

