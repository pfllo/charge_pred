\section{Experiments}
\subsection{Experimental Setup}
We use HanLP\footnote{\url{https://github.com/hankcs/HanLP}} for Chinese word segmentation and POS tagging. 
As for word embeddings, we use Baidu Encyclopedia, 3 million judgement documents and 3 million legal question answer pairs crawled from multiple legal forums as corpus, and the word2vec~\cite{mikolov2013distributed} for training. 
The size of the resultant word embedding is 100-d and there are 573,353 words in total. 
% During training and testing, all the time expressions, names and charges\footnote{Although rare, sometimes the charge may appear in the fact part. This conversion ensures that we do not use this information.} in the text are converted to 3 special tokens separately and the words not in the pre-trained word embeddings are converted to another special token. All the word embeddings remain unchanged during training except for the special tokens. 
We also randomly initialize a 50-d vector for each POS tag, which is concatenated with the word embedding to generate the final input.
Each GRU in the Bi-GRU is of size 75, the two full connection layers are of size 200 and 150.
The relevant article extractor generates top 20 articles, and the weight of the article attention loss ($\beta$ in Equation \ref{final_loss}) is 0.1. 
We use Stochastic Gradient Descent (SGD) for training, with learning rate 0.1, and batch size 8.
% We also clip the gradient so that the sum of square of the L2 norms of the trainable tensors does not exceed 5. 
% To accelerate the training, we constrain the number of sentences of each fact description to be less than 50 and the the maximum length of each fact sentence to be 50. As for articles, we constrain the maximum length of each sentence to be 30. 
The chi-square feature selector keeps the top 2,000 features.
% , and we use the SVM model implemented in scikit-learn \cite{scikit-learn} in our experiments.

We compare our full model with two variations: without article attention supervision and with only facts as input. We also implement a SVM model, which proves to be effective in \orange{many text classification tasks}~\cite{wang2012baselines,aletras2016predicting}. Specifically, the SVM model takes bag-of-words TF-IDF features as input, and uses chi-square to select top 2000 features.


\begin{table}
\centering
\small{
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model}				& \tabincell{c}{\textbf{Precision}} 	& \tabincell{c}{\textbf{Recall}} 		& \tabincell{c}{\textbf{F1}} 	\\
\hline
\textit{SVM} 				& \textbf{93.94}/79.53					& 77.66/49.54  					& 85.03/61.05 				 	\\
\hline
\textit{SVM\_article} 			& 91.77/71.33					& 72.10/45.85  					& 80.76/55.82				 	\\
\hline
\textit{NN}				& 91.30/\textbf{83.32}			& 87.39/74.99  					& 89.31/78.94					\\
\hline
\textit{NN\_article}			& 90.79/83.07					& 88.42/75.73  					& 89.59/79.23					\\
\hline
\textbf{\textit{NN\_supv\_article}} 	& 91.80/82.44 					& \textbf{88.67/78.62} 			& \textbf{90.21/80.48} 		 	\\
\hline
\hline
\textit{SVM\_gold\_article$^*$} 	& \textbf{98.97}/94.58			& 95.39/83.21  					& 97.15/88.53					\\
\hline
% \textit{SVM\_only\_gold$^*$} 		& 98.78/90.46					& 97.92/91.79  					& 98.35/91.12					\\
% \hline
\textbf{\textit{NN\_gold\_article$^*$}} 		& 98.78/\textbf{95.26} 			& \textbf{98.24/95.57} 			& \textbf{98.51/95.42} 			\\
\hline
\hline
\textit{NN\_article\_only} 			& 90.09/81.50				& 86.10/69.62				& 88.05/75.10		\\
\hline
\textit{NN\_gold\_only$^*$} 		& 97.22/92.39				& 98.36/94.73				& 97.79/93.55		\\
\hline
\end{tabular}
}
\caption{Charge prediction results. $^*$ refers to using gold standard (rather than extracted) articles during training and testing. Left side and right side of the slash refer to micro and macro statistics respectively.}
\label{tabble_main_results}
\end{table}


% \textit{NN\_article\_only} 			& 87.38/78.61				& 82.69/64.32				& 84.97/70.75		\\


\subsection{Charge Prediction Results}
\label{sec_main_results}
The charge prediction results are summarized in Table \ref{tabble_main_results}. 
% The left side of the slash refers to micro statistics, and the right side refers to macro statistics. 
% Micro precision (or recall) is the the number of correct predictions divided by the total number of predictions (or the total number of gold standard charges), while the macro precision (or recall) is the sum of the precision (or recall) of each charge divided by the number of distinct charges. The micro (or macro) F1 is the harmonic mean of the micro (or macro) precision and recall.
Macro precision (or recall) is calculated by averaging the precision (or recall) of each charge, and the micro precision (or recall) is caculated on all the data directly. In other words, macro statistics give equal weight to each charge, while micro statistics give equal weight to each individual prediction. The micro (or macro) F1 is the harmonic mean of the micro (or macro) precision and recall.


%The $Acc.$ refers to instance accuracy, which is the number of instances whose classes are all correctly predicted divided by the total number of instances.

We can see that, the basic \texttt{SVM} model, which only takes facts as input, proves to be a strong baseline. If we use both the facts and the gold standard articles of each case (\texttt{SVM\_gold\_article}), the performance can be further improved significantly, showing that the relevant law articles contain valuable information for charge prediction. However, the results become worse when we replace the gold standard articles with the extracted ones (\texttt{SVM\_article}), indicating that the SVM model cannot benefit from the additional information of relevant articles when they are noisy. 
% If we only use the gold standard articles of each case for classification (\texttt{SVM\_only\_gold}), the micro F1 can be further improved to 98.35, which further indicates that the SVM model has bad resistence to noise. \todo{is \texttt{\_only\_gold} necessary?}

On the other hand, since our attentive article aggregator has the ability to distinguish relevant articles from extraction mistakes, our neural network (NN) model (\texttt{NN\_article}) can make use of the noisy article extractions, and improve the performance over the model using only facts as input (\texttt{NN}), which has already ourperfroms \texttt{SVM} by a large margin.
Furthermore, if we use the gold standard articles to \orange{supervise} the article attention during training (\texttt{NN\_supv\_article}), the performance can be further improved.
Similar to SVM models, if we use gold standard articles (\texttt{NN\_gold\_article}) instead of extracted ones, there will exist a clear improvement as well, \orange{which actually indicates the upper bound of the improvement that relevant articles can bring}.
% and without surprise, it also outperforms \texttt{SVM\_gold\_article}.
Also note that, compared with the corresponding SVM models, the improvements made by our NN architecture are most prominent in macro statistics, showing that the SVM model has a \orange{strong} bias towards frequent charges, while the NN models have a more balanced performance between frequent and infrequent ones. 
\orange{This is probably due to the embedding methods used by our NN models. By using pre-trained word embeddings and training effective sentence-level and document-level sequence encoders, our model can capture the meaning of the cases belonging to infrequent charges even if their training data are limited, and thus performs better on these cases.}
% This is probably due to the nature of the BOW feature, which is unable to handle synonyms. However, understanding synonyms is crucial for inferring common patterns when training data are limited. Therefore, by using pre-trained word embeddings to handle synonyms, our NN model perfroms well on infrequent charges. 
% it can better understand the meaning of a word even when it is rare in the training data. 

% As for our neural network (NN) model (\texttt{NN}), since it can better understand the associations among sentences and words, and can select the most informative parts of the text, they significantly outperforms the corresponding SVM models. 

We also evaluate two variations which only use articles for charge prediction.
\texttt{NN\_gold\_only} uses concatenated gold standard articles, instead of facts, as input. 
\texttt{NN\_article\_only} uses extracted articles, and it still uses fact embedding and article aggregator to handle the noise in the top 20 articles, but only the aggregated article embedding $\mathbf{d}_a$ is used for charge prediction.
We can see that the performance of \texttt{NN\_gold\_only} is very good and \texttt{NN\_article\_only} also generates reasonable results. 
This actually indicates the nature of the civil law system that judgements are made based on statutory laws rather than decisions of previous cases.
Also note that \texttt{NN\_gold\_only} is not as good as \texttt{NN\_gold\_article}, showing that the facts can provide additional information for charge prediction even if we use gold standard law articles.
On the other hand, the performance of \texttt{NN} is also promising and outperforms \texttt{NN\_article\_only}, showing that although the judgments are based on statutory laws, we can still learn the \orange{legal logic} behind this implicitly through only the (facts, charges) paris. 

% Specifically, the relevant articles are concatenated and passed to the model by replacing the fact descriptions. We find that using only the top 20 extracted articles (\texttt{NN\_article\_only}) still generates \orange{fair} results, and the model performs very good when the extracted articles are replaced with gold standard ones (\texttt{NN\_gold\_only}), which significantly outperforms \texttt{NN}. 



% \orange{And the big gap between \texttt{NN\_supv\_article} and \texttt{NN\_gold\_article} indicates that there is still much room that our article extraction and embedding part can improve.}

% Recall that using only facts as input, the results of \texttt{NN} is promising, showing that although the legal system of China is mainly based on civil law system, the judgements are still consistent with the facts enough for our model to implicitly learn the legal logic behind the judgement.
% which to some extent indicates a reasonable consistency of Chinese judges in making judgements. 

\paragraph{Case Study}
We find that there exist some star-like confusion patterns among the charges in our dataset. For example, \emph{intentional injury} (\emph{pivot charges}) can be confused with multiple charges (\emph{peripheral charges}) like \emph{intentional homicide} (when the victim is dead, it differs from \emph{intentional injury} in whether the defendant intends to kill the victim) and \emph{picking quarrels and provoking troubles} (there may also exist injuries here). 
% We refer to charges can be confused with multiple charges as \emph{pivot charges}, and the charge often confused with a single charge as \emph{peripheral charges} here.
Since these charges share similar fact descriptions, e.g, how the injuries are caused, and the pivot charges are often relatively frequent charges, \texttt{SVM} performs bad in distinguishing these peripheral charges.  
However, by using GRU and attention mechanism, \texttt{NN} can attend to important details of the facts and significantly improves the performance on these peripheral charges. 
On the other hand, these learned patterns are not reliable enough, and lead to limited improvements on pivot charges.
As for \texttt{NN\_supv\_article}, it allows interactions between facts and law articles, and therefore learns more elaborated patterns, which significantly improve the pivot charges while less prominent improvements on other easily confused charges are made as well.

% On the other hand, while \texttt{NN\_supv\_article} also improves \texttt{NN} on these this kind of charges, a \orange{considerable} amount of improvements comes from the charges that can be confused with multiple charges, e.g., \emph{intentional injury} is also often confused with \emph{picking quarrels and provoking troubles} since there will also be injuries in the latter one, indicating that the additional information from relevant law articles can help the model handle the difficult charges better.


\subsection{Article Related Results}
\paragraph{Top $k$ Article Extraction}
% \begin{table}
% \centering
% \normalsize{
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% 				& \textbf{Top\_5} 	& \textbf{Top\_10} 		& \textbf{Top\_20} 	& \textbf{Top\_30} \\
% \hline
% \textit{Recall} 		& 77.60			& 88.96  				& 94.21			& 96.53 	\\
% \hline
% \textit{NDCG} 		& 80.28			& 84.32  				& 86.47			& 87.24 	\\
% \hline
% \end{tabular}
% }
% \caption{Top $k$ Article Extraction Performance}
% \label{tab_article_extraction}
% \end{table}
Our top $k$ article extractor achieves 77.60\%, 88.96\%, 94.21\% and 96.53\% recall with regard to the top 5, 10, 20 and 30 results respectively.
We can see that, although simple, our SVM article extractor already achieves promising performance, and the recall of the top 20 results is good enough for the following refinement components. 
On the other hand, the micro F1 of the extractor is only 61.08\% in the test set, which will lead to severe error propagation problem if we use the precition directly. Therefore, we instead retain the top 20 articles, and use the attention mechanism to resolve the noise in the top 20 results.

% Our top $k$ article extractor achieves 94.21\% recall for the top 20 results, which is good enough to for the following refinement components. However, the recall of the top 5 results is 77.60\% and the micro F1 is only 61.08\%, which indicates that we still cannot trust its prediction results directly, and therefore need to expand the number of results to ensure enough recall for the following components. Furtherfore, we find that the recall of the top 30 results is only 2.32\% higher than the top 20 results, therefore we use the top 20 results in this paper.

% Our relevant article extraction module achieves 86.44\% top 1 accuracy, 61.08\% micro F1, and the other evaluation indicators are shown in Table \ref{tab_article_extraction}. 
% We can see that, the relevant article extraction task is also a hard task in itself. Although our SVM model achieves reasonable ranking performance, its prediction performance is not very good. If we use the prediction results directly in our model, we will suffer from a severe error propagation problem. Therefore, we instead let the relevant article extraction module return the top $k$ articles, and use attention mechanism the distinguish true relevant articles from incorrect ones. Since the recall of the top 20 results has achieved 94.21\%, which already includes most of the relevant articles, we use top 20 articles in our experiment.

\begin{table}
\centering
\normalsize{
\begin{tabular}{|c|c|c|c|c|}
\hline
% \textbf{Att\_Weight}
% \bm{$\beta$}
\bm{$\beta$}							& \textbf{Prec@1} 		& \textbf{MAP} 			& \textbf{Charge\_F1} \\
\hline
\textit{0} 								& 60.94								& 61.61 						& 89.59/79.23 	\\
\hline
\textit{0.01} 						& 81.06								& 78.00							& 89.77/79.48 	\\
\hline
\textit{0.1} 							& 87.90								& 83.39							& \textbf{90.21/80.48} 	\\
\hline
\textit{1} 								& \textbf{92.66}			& \textbf{88.24}		& 89.83/78.66 	\\
\hline
\end{tabular}
}
\caption{Article Attention Performance}
\label{tab_article_att}
\end{table}


\paragraph{Article Attention}
We can consider the article attention module as a re-ranking function over the extracted top $k$ articles, and accordingly use the gold standard articles in the $k$ articles to evaluate the re-ranking performance. Table \ref{tab_article_att} shows the model performance on article attention (column 2-3) and charge prediction \orange{(colum 4), under different} article attention loss weights ($\beta$ in Equation \ref{final_loss}). \texttt{Prec@1} refers to top 1 precision, and \texttt{MAP} refers to mean average precision.

We can see that, even if there is no supervision over the article attention ($\beta=0$), our model still has reasonable performance on re-ranking the $k$ articles. When attention supervision is added, the attention quality improves significantly, and it keeps increasing as $\beta$ goes up.
The \orange{fair} performance of the article attention indicates that our model can provide reasonable legal basis to support the charge prediction.
However, the charge prediction performance does not always increase with the article attention quality, and the best performance is achieved when $\beta=0.1$. This shows that there exists a tradeoff between the benefits of more accurate article attention and the less model capacity left for charge classification due to the increased emphasis on the article attention performance.

\begin{table}
\centering
\small{
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model}												& \textbf{Precision} 				& \textbf{Recall} 				& \textbf{F1} 	\\
\hline
\textit{SVM} 													& \textbf{100.00}						& 40.20  									& 57.34 				 	\\
\hline
\textit{NN}														& 87.14											& 59.80 									& 70.93					\\
\hline
\textit{NN\_article}					& 87.18											& 66.67 									& 75.56					\\
\hline
\textbf{\textit{NN\_supv\_article}} 	& 90.00 										& \textbf{70.59} 					& \textbf{79.12} 		 	\\
\hline
\end{tabular}
}
\caption{Micro Statistics on News Data}
\label{tabble_news_results}
\end{table}

\subsection{Performance on News Data}
Since there are some differences between the expressions used by \orange{laymen} and legal practitioners, 
we manually annotated 100 news\footnote{From \url{news.cn/legal} and \url{legal.people.com.cn}}, 
which contains 261.79 words on average and 25 distinct charges, to see how the model trained on judgement documents performs on the fact descriptions written by ordinary people. The results are shown in Table \ref{tabble_news_results}.
Also note that the size of the news test set is relatively small with regard to the number of charges, which results in less reliable macro statistics, we therefore only report micro statistics here.

We can see that, although \texttt{SVM} has good performance on the judgement document data, it suffers from a significant performance drop on news data.
Recall that \texttt{SVM} is based on BOW, this performance drop actually indicates an innegligible gap in expressions used by laymen and legal practitioners. 
However, the high micro precision of \texttt{SVM} indicates that, although cannot generalizes well, the patterns learned by the SVM model are very reliable in themselves.
As for our NN models, although there also exists a performance drop, they are much better than the SVM model. This shows that the  gap in expressions can be resolved by using embedding methods to a some extent. 
For example, the word 暴打$\ $(beat up) is seldom used in judgement documents, making it hard for the SVM model to correctly classify the news containing this word. However, by using pre-trained word embeddings, our NN models can understand that this word is similar to 殴打$ $(beat), and can therefore obtain useful information from it.
Furthermore, we can see that the model using relevant articles (\texttt{\_article}) clearly outperforms \texttt{NN}, \orange{showing that relevant articles also provide valuable information in this situation.}

% Also note that, since the news test set is relatively small regarding the number of charges, the macro statistics are not as meaningful as those in the judgement document test set. For example, correctly predicting a charge with only one instance will improve the macro precision and recall by 4\% (${1}\div{25}$). Therefore, \texttt{NN\_article} outperforms \texttt{NN\_supv\_article} in macro F1 by 4.38\% does not necessarily mean that it performs better on infrequent charges. 
% Actually, %except for other differences, 
% \texttt{NN\_article} only generates one more such case than \texttt{NN\_supv\_article}, while \texttt{NN\_supv\_article} correctly predicts 4 more cases than \texttt{NN\_article} in total. Therefore, we still consider \texttt{NN\_supv\_article} to be better than \texttt{NN\_article} on news data.


% Also note that compared with \texttt{NN\_article}, \texttt{NN\_supv\_article} has better micro statistics but worse macro statistics, indicating that the additional supervision over article attention makes the model perform better on frequent charges but worse on infrequent ones on news data. 
% The reason is that, since the top $k$ article extractor also utilizes BOW features, its performances drops significantly on news data, leading to some situations where the key article is not included in the top $k$ results. While the attention of \texttt{NN\_article} will be diffusive in these situations, due to the article attention supervision, \texttt{NN\_supv\_article} has a stronger tendency to put the attention on the articles related to frequent or other relevant charges, and thus is more likely to generate false prediction.  
% \todo{re-organize this part}

